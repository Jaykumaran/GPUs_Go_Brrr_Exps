{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Performance Analysis of Concurrent Model Execution on Separate GPUs\n\n**Reference:** [https://discuss.pytorch.org/t/assign-two-models-to-two-gpus-and-run-concurrently/150815/2](https://discuss.pytorch.org/t/assign-two-models-to-two-gpus-and-run-concurrently/150815/2)\n\n**Experiment Setup:** `1000 Forward Process, no backward or optimization`\n\n**Individual Model Execution Times:**\n\n*   **DenseNet-121:** Took ~35.73 seconds to process 1000 forward passes.\n*   **ResNet-101:** Took ~62.80 seconds to process 1000 forward passes.\n    *   ResNet-101 is significantly slower than DenseNet-121.\n\n**Total Parallel Execution Time:**\n\n*   ~63.79 seconds, which is close to the time taken by the slower model (ResNet-101).\n    *   This is because parallel execution completes when the longest task finishes.\n\n**Comparison to Sequential Execution:**\n\n*   If executed sequentially (one after another), the total time would be:\n    ```\n    35.73s + 62.80s = ~98.52s\n    ```\n*   By running concurrently on separate GPUs, the execution time was reduced by ~35.4%, achieving a speedup of ~1.54×.\n\n**Performance Bottleneck:**\n\n*   The execution time is primarily dictated by ResNet-101, since `ThreadPoolExecutor` runs both models in parallel, but the script only completes when the slowest model is done.\n\n**Potential for Further Speedup:**\n\n*   Using CUDA streams instead of Python threads might further reduce overhead.\n*   Profiling could reveal whether GPU memory access or compute bottlenecks are causing the disparity in execution times.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nfrom concurrent.futures import ThreadPoolExecutor, wait\nimport time\n\ndevice_0 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice_1 = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n\nstart_time = time.time()\n\ndense = torchvision.models.densenet121(pretrained=False).to(device_0)\nrest = torchvision.models.resnet101(pretrained=False).to(device_1)\n\ndummy1 = torch.ones((200,3,32,32)).to(device_0)\ndummy2 = torch.ones((200,3,32,32)).to(device_1)\n\ndef train(model,value,epoch):\n    ti = time.perf_counter()\n    for _ in range(epoch):\n        ret = model(value)\n    to = time.perf_counter()\n    return ret.shape, to-ti\n\noutput = []\n\nwith ThreadPoolExecutor() as executor:\n    futures = []\n    futures.append(executor.submit(train, dense, dummy1, 1000))\n    futures.append(executor.submit(train, rest, dummy2, 1000))\n    complete_futures, incomplete_futures = wait(futures)    # waits untils both the processes are completed\n    for f in complete_futures:\n        output.append(f.result())\n        print(str(f.result()))\n\nelapsed = (time.time() - start_time)\nprint(f\"Total time of execution {round(elapsed, 4)} second(s)\")\nprint(\"Output is:\",output)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-14T04:58:11.944046Z","iopub.execute_input":"2025-03-14T04:58:11.944308Z","iopub.status.idle":"2025-03-14T04:59:23.347739Z","shell.execute_reply.started":"2025-03-14T04:58:11.944283Z","shell.execute_reply":"2025-03-14T04:59:23.346765Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"(torch.Size([200, 1000]), 37.140874870000005)\n(torch.Size([200, 1000]), 63.63703838299995)\nTotal time of execution 64.9472 second(s)\nOutput is: [(torch.Size([200, 1000]), 37.140874870000005), (torch.Size([200, 1000]), 63.63703838299995)]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nfrom concurrent.futures import ThreadPoolExecutor, wait\nimport time\n\ndevice_0 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# device_1 = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n\nstart_time = time.time()\n\ndense = torchvision.models.densenet121(pretrained=False).to(device_0)\n# rest = torchvision.models.resnet101(pretrained=False).to(device_0)\n\ndummy1 = torch.ones((200,3,32,32)).to(device_0)\n# dummy2 = torch.ones((200,3,32,32)).to(device_0)\n\ndef train(model,value,epoch):\n    ti = time.perf_counter()\n    for _ in range(epoch):\n        ret = model(value)\n    to = time.perf_counter()\n    return ret.shape, to-ti\n\noutput = []\n\n\ntrain(dense, dummy1, 1000)\n\nelapsed = (time.time() - start_time)\nprint(f\"Total time of execution {round(elapsed, 4)} second(s)\")\n# print(\"Output is:\",output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T04:59:23.348408Z","iopub.execute_input":"2025-03-14T04:59:23.348706Z","iopub.status.idle":"2025-03-14T04:59:48.208353Z","shell.execute_reply.started":"2025-03-14T04:59:23.348687Z","shell.execute_reply":"2025-03-14T04:59:48.207475Z"}},"outputs":[{"name":"stdout","text":"Total time of execution 24.8542 second(s)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nfrom concurrent.futures import ThreadPoolExecutor, wait\nimport time\n\ndevice_0 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# device_1 = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n\nstart_time = time.time()\n\n# dense = torchvision.models.densenet121(pretrained=False).to(device_0)\nrest = torchvision.models.resnet101(pretrained=False).to(device_1)\n\n# dummy1 = torch.ones((200,3,32,32)).to(device_0)\ndummy2 = torch.ones((200,3,32,32)).to(device_1)\n\ndef train(model,value,epoch):\n    ti = time.perf_counter()\n    for _ in range(epoch):\n        ret = model(value)\n    to = time.perf_counter()\n    return ret.shape, to-ti\n\noutput = []\n\ntrain(rest, dummy2, 1000)\n\nelapsed = (time.time() - start_time)\nprint(f\"Total time of execution {round(elapsed, 4)} second(s)\")\nprint(\"Output is:\",output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T05:02:04.302387Z","iopub.execute_input":"2025-03-14T05:02:04.302608Z","iopub.status.idle":"2025-03-14T05:03:12.584492Z","shell.execute_reply.started":"2025-03-14T05:02:04.302579Z","shell.execute_reply":"2025-03-14T05:03:12.583344Z"}},"outputs":[{"name":"stdout","text":"Total time of execution 68.2755 second(s)\nOutput is: []\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Analysis of Profiling Results for **One Epoch**\n\nYour profiling output provides key insights into the execution performance of DenseNet-121 and ResNet-101 on separate GPUs.\n\n| Metric                | DenseNet-121 (GPU 0) | ResNet-101 (GPU 1) |\n| --------------------- | -------------------- | ------------------- |\n| Total CUDA Time       | 49.75 ms             | 52.48 ms            |\n| Total CPU Time        | 50.39 ms             | 41.57 ms            |\n| Batch Norm CUDA Time  | 7.62 ms              | 4.43 ms             |\n| Convolution CUDA Time | 23.20 ms             | 44.73 ms            |\n| Memory Usage          | 513.29 MB            | 500.89 MB           |\n\n### 2️⃣ Why is ResNet-101 Slower?\n\n**Heavy Convolution Overhead**\n\n*   ResNet-101 spends 86.42% of CUDA time (44.73ms) on `aten::cudnn_convolution`, whereas DenseNet-121 only spends 65.33% (23.20ms).\n*   This means that ResNet-101's deep convolutional layers are the biggest performance bottleneck.\n\n**SGEMM Kernel Operations**\n\n*   ResNet-101 shows heavy CUDA activity in `volta_sgemm_64x64_nn` and `volta_sgemm_128x64_nn` kernels (Total: 28.2ms).\n*   These are matrix-multiplication operations used in convolutions, which contribute significantly to processing time.\n\n**Batch Normalization Impact**\n\n*   DenseNet-121 spends 7.62ms on batch normalization, whereas ResNet-101 spends only 4.42ms.\n*   This suggests DenseNet-121 has more frequent or larger batch norm operations, but they are not a major bottleneck.\n\n**Memory Usage**\n\n*   DenseNet-121: 513.29MB\n*   ResNet-101: 500.89MB\n*   Memory allocation is comparable, meaning the difference in speed is likely due to computation complexity rather than memory bandwidth.\n\n### 3️⃣ How to Optimize Performance?\n\n**✅ Use Mixed Precision (AMP) to Speed Up Convolutions**\n\n*   ResNet-101 spends a lot of time in SGEMM operations and convolutions, which can be optimized using automatic mixed precision (AMP).\n*   Try this in your training loop:\n\n    ```python\n    with torch.cuda.amp.autocast():\n        output = model(input)\n    ```\n    *   This can reduce computational cost and improve speed with minimal loss of accuracy.\n\n**✅ Use CUDA Streams for Better Parallelism**\n\n*   Instead of using `ThreadPoolExecutor`, try CUDA streams to overlap computation:\n\n    ```python\n    stream0 = torch.cuda.Stream(device_0)\n    stream1 = torch.cuda.Stream(device_1)\n\n    with torch.cuda.stream(stream0):\n        output1 = dense(dummy1)\n\n    with torch.cuda.stream(stream1):\n        output2 = rest(dummy2)\n    ```\n    *   This allows both models to run in parallel without blocking.\n\n**✅ Increase Batch Size (If GPU Memory Allows)**\n\n*   Since memory usage is similar (~500MB), try increasing batch size to improve GPU utilization.\n*   Example:\n\n    ```python\n    dummy1 = torch.ones((400,3,32,32)).to(device_0)  # Increase batch size\n    dummy2 = torch.ones((400,3,32,32)).to(device_1)\n    ```\n    *   If GPU memory can handle it, this reduces batch normalization and convolution overhead per sample.\n\n### 4️⃣ Summary\n\n*   ResNet-101 is slower mainly due to convolution overhead (44.73ms CUDA time vs. 23.20ms in DenseNet-121).\n*   SGEMM operations (matrix multiplications) in ResNet-101 take significant time.\n*   Memory usage is similar, meaning performance bottlenecks are computational rather than memory-related.\n*   Optimization suggestions:\n    *   Enable Mixed Precision (AMP) to optimize convolutions.\n    *   Use CUDA streams instead of CPU threads for true parallelism.\n    *   Increase batch size to improve GPU utilization.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nimport time\nimport torch.profiler\n\n# Assign devices\ndevice_0 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice_1 = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n\n# Load models\ndense = torchvision.models.densenet121(pretrained=False).to(device_0)\nrest = torchvision.models.resnet101(pretrained=False).to(device_1)\n\n# Create dummy input tensors\ndummy1 = torch.ones((200,3,32,32)).to(device_0)\ndummy2 = torch.ones((200,3,32,32)).to(device_1)\n\n# Profiling function for 1000 epochs\ndef profile_model(model, input_tensor, device, model_name, epochs=1000):\n    print(f\"\\nProfiling {model_name} on {device} for {epochs} epochs...\\n\")\n\n    with torch.profiler.profile(\n        activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n        schedule=torch.profiler.schedule(\n            wait=5,   # Skip first 5 iterations (warm-up)\n            warmup=10,  # Profile next 10 iterations\n            active=20,  # Collect profiling for 20 iterations\n            repeat=5  # Repeat profiling 5 times during execution\n        ),\n        record_shapes=True,\n        with_stack=True,\n        profile_memory=True\n    ) as prof:\n        torch.cuda.synchronize(device)  # Ensure all previous operations are done\n        start_time = time.perf_counter()\n\n        for epoch in range(epochs):\n            with torch.profiler.record_function(\"Model Forward Pass\"):\n                _ = model(input_tensor)  # Run forward pass\n            \n            prof.step()  # Step the profiler at each iteration\n        \n        torch.cuda.synchronize(device)  # Ensure all GPU operations are finished\n        end_time = time.perf_counter()\n    \n    execution_time = round(end_time - start_time, 4)\n    print(f\"{model_name} execution time for {epochs} epochs: {execution_time}s\")\n\n    # Print top profiling events\n    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n    \n    # Save profiling results\n    prof.export_chrome_trace(f\"{model_name}_profile.json\")\n    print(f\"Profile saved: {model_name}_profile.json\")\n\n    return execution_time\n\n# Profile DenseNet-121\ndense_time = profile_model(dense, dummy1, device_0, \"DenseNet-121\", epochs=1000)\n\n# Profile ResNet-101\nrest_time = profile_model(rest, dummy2, device_1, \"ResNet-101\", epochs=1000)\n\n# Print Summary\nprint(\"\\n====== Execution Summary ======\")\nprint(f\"DenseNet-121 Execution Time for 1000 epochs: {dense_time}s\")\nprint(f\"ResNet-101 Execution Time for 1000 epochs: {rest_time}s\")\nprint(\"Profiling complete! 🚀\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T05:03:12.585495Z","iopub.execute_input":"2025-03-14T05:03:12.585734Z","iopub.status.idle":"2025-03-14T05:05:40.009168Z","shell.execute_reply.started":"2025-03-14T05:03:12.585714Z","shell.execute_reply":"2025-03-14T05:05:40.008274Z"}},"outputs":[{"name":"stdout","text":"\nProfiling DenseNet-121 on cuda:0 for 1000 epochs...\n\nDenseNet-121 execution time for 1000 epochs: 46.2665s\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                     Model Forward Pass         0.00%       0.000us         0.00%       0.000us       0.000us     953.343ms       270.94%     953.343ms      47.667ms           0 b           0 b           0 b           0 b            20  \n                                     Model Forward Pass        30.68%     301.168ms        99.81%     979.596ms      48.980ms       0.000us         0.00%     351.859ms      17.593ms           0 b           0 b           0 b     -10.80 Gb            20  \n                                           aten::conv2d         1.25%      12.299ms        23.07%     226.388ms      94.328us       0.000us         0.00%     194.941ms      81.225us           0 b           0 b       2.17 Gb           0 b          2400  \n                                      aten::convolution         2.48%      24.375ms        21.81%     214.090ms      89.204us       0.000us         0.00%     194.941ms      81.225us           0 b           0 b       2.17 Gb           0 b          2400  \n                                     aten::_convolution         1.85%      18.136ms        19.33%     189.714ms      79.048us       0.000us         0.00%     194.941ms      81.225us           0 b           0 b       2.17 Gb           0 b          2400  \n                                aten::cudnn_convolution        10.91%     107.089ms        17.48%     171.578ms      71.491us     194.941ms        55.40%     194.941ms      81.225us           0 b           0 b       2.17 Gb       2.17 Gb          2400  \n                                       aten::batch_norm         0.66%       6.465ms        26.18%     256.968ms     106.185us       0.000us         0.00%      86.231ms      35.633us           0 b           0 b       5.02 Gb           0 b          2420  \n                           aten::_batch_norm_impl_index         1.17%      11.457ms        25.52%     250.503ms     103.514us       0.000us         0.00%      86.231ms      35.633us           0 b           0 b       5.02 Gb           0 b          2420  \n                                 aten::cudnn_batch_norm        11.30%     110.872ms        24.36%     239.046ms      98.779us      86.231ms        24.51%      86.231ms      35.633us           0 b           0 b       5.02 Gb           0 b          2420  \nvoid cudnn::bn_fw_tr_1C11_singleread<float, 512, tru...         0.00%       0.000us         0.00%       0.000us       0.000us      54.434ms        15.47%      54.434ms      25.437us           0 b           0 b           0 b           0 b          2140  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 981.487ms\nSelf CUDA time total: 351.859ms\n\nProfile saved: DenseNet-121_profile.json\n\nProfiling ResNet-101 on cuda:1 for 1000 epochs...\n\nResNet-101 execution time for 1000 epochs: 78.2037s\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                     Model Forward Pass         0.00%       0.000us         0.00%       0.000us       0.000us        1.335s        93.14%        1.335s      66.744ms           0 b           0 b           0 b           0 b            20  \n                                     Model Forward Pass        18.48%     246.219ms        99.93%        1.332s      66.579ms       0.000us         0.00%        1.323s      66.149ms           0 b           0 b           0 b     -10.22 Gb            20  \n                                           aten::conv2d         0.76%      10.185ms        34.08%     454.069ms     218.303us       0.000us         0.00%        1.140s     548.105us           0 b           0 b       4.96 Gb           0 b          2080  \n                                      aten::convolution         1.62%      21.565ms        33.31%     443.884ms     213.406us       0.000us         0.00%        1.140s     548.105us           0 b           0 b       4.96 Gb           0 b          2080  \n                                     aten::_convolution         1.16%      15.521ms        31.69%     422.319ms     203.038us       0.000us         0.00%        1.140s     548.105us           0 b           0 b       4.96 Gb           0 b          2080  \n                                aten::cudnn_convolution         7.38%      98.354ms        30.53%     406.798ms     195.576us        1.140s        79.54%        1.140s     548.105us           0 b           0 b       4.96 Gb       4.96 Gb          2080  \n                                   volta_sgemm_64x64_nn         0.00%       0.000us         0.00%       0.000us       0.000us     684.302ms        47.75%     684.302ms       1.313ms           0 b           0 b           0 b           0 b           521  \n                                  volta_sgemm_128x64_nn         0.00%       0.000us         0.00%       0.000us       0.000us     130.787ms         9.13%     130.787ms     241.304us           0 b           0 b           0 b           0 b           542  \n                                       aten::batch_norm         0.41%       5.468ms        30.40%     405.039ms     194.730us       0.000us         0.00%     111.795ms      53.748us           0 b           0 b       5.00 Gb           0 b          2080  \n                           aten::_batch_norm_impl_index         0.75%      10.048ms        29.99%     399.571ms     192.101us       0.000us         0.00%     111.795ms      53.748us           0 b           0 b       5.00 Gb           0 b          2080  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 1.332s\nSelf CUDA time total: 1.433s\n\nProfile saved: ResNet-101_profile.json\n\n====== Execution Summary ======\nDenseNet-121 Execution Time for 1000 epochs: 46.2665s\nResNet-101 Execution Time for 1000 epochs: 78.2037s\nProfiling complete! 🚀\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### CUDA Stream","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nimport time\n\n# Assign devices\ndevice_0 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice_1 = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n\n# Load models\ndense = torchvision.models.densenet121(pretrained=False).to(device_0)\nrest = torchvision.models.resnet101(pretrained=False).to(device_1)\n\n# Create dummy input tensors (Increased batch size for better GPU utilization)\ndummy1 = torch.ones((400,3,32,32)).to(device_0)  # Increased batch size\ndummy2 = torch.ones((400,3,32,32)).to(device_1)\n\n# Create CUDA Streams for Parallel Execution\nstream0 = torch.cuda.Stream(device_0)\nstream1 = torch.cuda.Stream(device_1)\n\n# Function to Run Model with Mixed Precision (AMP) for 1000 Epochs\ndef train_with_amp(model, input_tensor, stream, device, model_name, epochs=1000):\n    torch.cuda.synchronize(device)  # Ensure all previous operations are done\n    print(f\"Running {model_name} on {device} for {epochs} epochs...\")\n\n    with torch.cuda.stream(stream):  # Assign computation to the stream\n        with torch.autocast(device_type='cuda', dtype=torch.float16):  # Enable Mixed Precision\n            start_time = time.perf_counter()\n            for _ in range(epochs):  # Run for 1000 epochs\n                output = model(input_tensor)  # Forward Pass\n            torch.cuda.synchronize(device)  # Sync GPU operations\n            end_time = time.perf_counter()\n    \n    execution_time = round(end_time - start_time, 4)\n    print(f\"{model_name} execution time for {epochs} epochs: {execution_time}s\")\n    return output.shape, execution_time\n    \n# Run both models in parallel using CUDA Streams\ndense_result = train_with_amp(dense, dummy1, stream0, device_0, \"DenseNet-121\", epochs=1000)\nrest_result = train_with_amp(rest, dummy2, stream1, device_1, \"ResNet-101\", epochs=1000)\n\n# Print final results\nprint(\"\\n====== Execution Summary ======\")\nprint(f\"DenseNet-121 Output Shape: {dense_result[0]}, Execution Time: {dense_result[1]}s\")\nprint(f\"ResNet-101 Output Shape: {rest_result[0]}, Execution Time: {rest_result[1]}s\")\nprint(\"Execution complete! 🚀\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T05:05:40.010652Z","iopub.execute_input":"2025-03-14T05:05:40.010860Z","iopub.status.idle":"2025-03-14T05:06:41.814375Z","shell.execute_reply.started":"2025-03-14T05:05:40.010844Z","shell.execute_reply":"2025-03-14T05:06:41.813633Z"}},"outputs":[{"name":"stdout","text":"Running DenseNet-121 on cuda:0 for 1000 epochs...\nDenseNet-121 execution time for 1000 epochs: 32.2444s\nRunning ResNet-101 on cuda:1 for 1000 epochs...\nResNet-101 execution time for 1000 epochs: 28.7406s\n\n====== Execution Summary ======\nDenseNet-121 Output Shape: torch.Size([400, 1000]), Execution Time: 32.2444s\nResNet-101 Output Shape: torch.Size([400, 1000]), Execution Time: 28.7406s\nExecution complete! 🚀\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}